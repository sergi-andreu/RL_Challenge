{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Colab for training a ray agent (default: PPO) and save checkpoints to drive"
      ],
      "metadata": {
        "id": "Vno7wTLF_o0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the gym platform environment and required libs"
      ],
      "metadata": {
        "id": "7dD2d_ia_yEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha0z7T_KGOud",
        "outputId": "57c8175b-7bd8-4a09-c668-d6d9dc1840f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining gym_platform from git+https://github.com/cycraig/gym-platform#egg=gym_platform\n",
            "  Updating ./src/gym-platform clone\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q e9329879dbb62badbbef89648162c97ba1e4d837\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym_platform) (0.23.1)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.7/dist-packages (from gym_platform) (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gym_platform) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym_platform) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym_platform) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym_platform) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym->gym_platform) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym->gym_platform) (4.1.1)\n",
            "Installing collected packages: gym-platform\n",
            "  Attempting uninstall: gym-platform\n",
            "    Found existing installation: gym-platform 0.0.1\n",
            "    Can't uninstall 'gym-platform'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-platform\n",
            "Successfully installed gym-platform-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.17.3)\n",
            "Requirement already satisfied: grpcio<=1.43.0,>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.43.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (22.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.2.0)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (20.16.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (2.23.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.8.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (6.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.1.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.7.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.8.10)\n",
            "Requirement already satisfied: gym<0.24.0,>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.23.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.18.3)\n",
            "Requirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.2.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (2.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.3.5)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (4.0.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.32.0->ray[rllib]) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym<0.24.0,>=0.21.0->ray[rllib]) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->gym<0.24.0,>=0.21.0->ray[rllib]) (3.9.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (3.0.9)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[rllib]) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[rllib]) (5.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[rllib]) (2022.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (1.24.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.9.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray[rllib]) (2.5.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray[rllib]) (0.3.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.7/dist-packages (4.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -e git+https://github.com/cycraig/gym-platform#egg=gym_platform\n",
        "!pip install ray[rllib]\n",
        "!pip install lz4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount google drive to save checkpoints there"
      ],
      "metadata": {
        "id": "XI-OZUwx_6nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19p5k2arN9G1",
        "outputId": "0274e247-0812-4a5d-c693-7ada4e7c0dd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Weights&Biases\n",
        "(Not used)"
      ],
      "metadata": {
        "id": "c-znoKy2ABnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gTCAjzX3d3l",
        "outputId": "26e39de9-3e57-4507-a83e-81a17f93b5d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required libraries for environment and agents"
      ],
      "metadata": {
        "id": "OTJhcZZyAN3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_platform\n",
        "from gym_platform.envs.platform_env import PlatformEnv\n",
        "import ray\n",
        "from ray.tune.registry import register_env\n",
        "import ray.rllib.agents.ppo as ppo\n",
        "import ray.rllib.agents.impala as impala\n",
        "import ray.rllib.agents.ddpg as ddpg\n",
        "import shutil\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM0X_HtvGoPi",
        "outputId": "6895750a-fcc7-454d-da5e-4c867c1d7ec6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "2022-10-23 17:50:09,028\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.agents.impala` has been deprecated. Use `ray.rllib.algorithms.impala` instead. This will raise an error in the future!\n",
            "2022-10-23 17:50:09,057\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.agents.ddpg` has been deprecated. Use `ray.rllib.algorithms.ddpg` instead. This will raise an error in the future!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the training function"
      ],
      "metadata": {
        "id": "9-vPo0BCATNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "# Choose the ppo algorithm\n",
        "algorithm = \"ppo\"\n",
        "#algorithm = \"impala\"\n",
        "#algorithm = \"td3\"\n",
        "\n",
        "def train_agent(env_cur, n_episodes):\n",
        "    '''\n",
        "    Train the agent\n",
        "        Parameters:\n",
        "            env_cur: gym environment\n",
        "            n_episodes: total number of training episodes\n",
        "        Returns:\n",
        "            agent: RL agent\n",
        "            sav_file: last training checkpoint file saved\n",
        "    '''\n",
        "\n",
        "    save_dir = \"train_dir\"\n",
        "    shutil.rmtree(save_dir, ignore_errors=True, onerror=None)\n",
        "\n",
        "    ray.init()\n",
        "    register_env(env_cur, lambda config: PlatformEnv())\n",
        "\n",
        "    if algorithm == \"ppo\":\n",
        "      config = ppo.DEFAULT_CONFIG.copy()\n",
        "    elif algorithm == \"impala\":\n",
        "      config = impala.DEFAULT_CONFIG.copy()\n",
        "    elif algorithm == \"td3\":\n",
        "      config = ddpg.td3.TD3_DEFAULT_CONFIG.copy()\n",
        "\n",
        "    config[\"framework\"] = \"torch\"\n",
        "\n",
        "    # Make custom fcnet model\n",
        "\n",
        "    config[\"log_level\"] = \"WARN\"\n",
        "    if algorithm == \"ppo\":\n",
        "        config[\"lr\"] = 5e-6\n",
        "        config[\"lr_schedule\"] = [0, 5e-5], [200000, 1e-6] #[0, lr_start], [lr_time, lr_end]\n",
        "        config[\"model\"] =   {\"fcnet_hiddens\": [512, 512, 512], \"fcnet_activation\": \"relu\"}\n",
        "\n",
        "    #config[\"sgd_minibatch_size\"] = 128 #128\n",
        "    #config[\"train_batch_size\"] = 4000 #4000\n",
        "    #config[\"num_gpus\"] = 1\n",
        "\n",
        "    print(config)\n",
        "\n",
        "    if algorithm == \"ppo\":\n",
        "      agent = ppo.PPOTrainer(config, env=env_cur)\n",
        "    elif algorithm == \"impala\":\n",
        "      agent = impala.ImpalaTrainer(config, env=env_cur)\n",
        "    elif algorithm == \"td3\":\n",
        "      agent = ddpg.TD3Trainer(config, env=env_cur)\n",
        "\n",
        "    env = gym.make(env_cur)\n",
        "    policy = agent.get_policy() \n",
        "\n",
        "    # train agent\n",
        "    for i in range(n_episodes):\n",
        "        start_time = time.time()\n",
        "\n",
        "        result = agent.train()\n",
        "        sav_file = agent.save(save_dir)\n",
        "        print(f'{i}: reward: mean={result[\"episode_reward_mean\"]}, '\n",
        "              f'max={result[\"episode_reward_max\"]}, min={result[\"episode_reward_min\"]}. '\n",
        "              f'length={result[\"episode_len_mean\"]}. file={sav_file}, '\n",
        "              #f\"lr={result['info']['learner']['default_policy']}\")\n",
        "              f\"lr={result['info']['learner']['default_policy']['learner_stats']['cur_lr']}\")\n",
        "        \n",
        "        if i%10 == 0: agent.save(\"drive/MyDrive/RL/\")\n",
        "        \n",
        "    return agent, sav_file\n",
        "\n",
        "\n",
        "def view_agent(env_cur, agent, sav_file):\n",
        "    '''\n",
        "    Display the trained agent\n",
        "        Parameters:\n",
        "            env_cur: gym environment\n",
        "            agent: trained RL agent\n",
        "            sav_file: checkpoint file saved from training\n",
        "        Returns:\n",
        "            None\n",
        "    '''\n",
        "\n",
        "    # run the policy\n",
        "    agent.restore(sav_file)\n",
        "    env = gym.make(env_cur)\n",
        "\n",
        "    obs = env.reset()\n",
        "    ttl_reward = 0\n",
        "    n_step = 10\n",
        "\n",
        "    for step in range(n_step):\n",
        "        action = agent.compute_action(obs)\n",
        "\n",
        "        print(\"Action\", action)\n",
        "\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        ttl_reward += reward\n",
        "\n",
        "        env.render()\n",
        "        time.sleep(0.001)\n",
        "        if done:\n",
        "            # reward at the end of episode\n",
        "            print(\"reward\", ttl_reward)\n",
        "            obs = env.reset()\n",
        "            ttl_reward = 0\n",
        "            \n",
        "    env.close()"
      ],
      "metadata": {
        "id": "MaQRUkMLHjTF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ray.shutdown()\n",
        "def main():\n",
        "    training_episodes = 100 #1000\n",
        "    env_cur = \"Platform-v0\"\n",
        "    agent, sav_file = train_agent(env_cur, training_episodes)\n",
        "    #view_agent(env_cur, agent, sav_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_GXYvo2HvRc",
        "outputId": "66752d60-6363-4911-c5ab-6cc88fff031a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-10-23 17:51:02,175\tINFO worker.py:1421 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 2, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-06, 'train_batch_size': 4000, 'model': {'fcnet_hiddens': [512, 512, 512], 'fcnet_activation': 'relu'}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'lr_schedule': ([0, 5e-05], [200000, 1e-06]), 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m /usr/local/lib/python3.7/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /usr/local/lib/python3.7/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   import imp\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1257)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=1256)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1257)\u001b[0m /usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1257)\u001b[0m   logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1256)\u001b[0m /usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1256)\u001b[0m   logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "\u001b[2m\u001b[36m(RolloutWorker pid=1256)\u001b[0m 2022-10-23 17:51:10,588\tWARNING env.py:143 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
            "2022-10-23 17:51:12,176\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
            "/usr/local/lib/python3.7/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuple(Box(0.0, 1.0, (9,), float32), Discrete(200))\n",
            "Model summary FullyConnectedNetwork(\n",
            "  (_logits): SlimFC(\n",
            "    (_model): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=9, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (_hidden_layers): Sequential(\n",
            "    (0): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=209, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (1): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (2): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (_value_branch_separate): Sequential(\n",
            "    (0): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=209, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (1): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (2): SlimFC(\n",
            "      (_model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (_value_branch): SlimFC(\n",
            "    (_model): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "0: reward: mean=0.14210793248125744, max=0.7843337829019648, min=0.0. length=2.1936368623148654. file=train_dir/checkpoint_000001, lr=5.0000000000000016e-05\n",
            "1: reward: mean=0.16713297303320532, max=0.7985370936190745, min=0.0. length=2.423636363636364. file=train_dir/checkpoint_000002, lr=4.902e-05\n",
            "2: reward: mean=0.20345393748193633, max=0.8850818585964672, min=0.0. length=2.626149802890933. file=train_dir/checkpoint_000003, lr=4.803999999999999e-05\n",
            "3: reward: mean=0.25141719421962905, max=0.9179142489142738, min=0.0. length=2.838412473423104. file=train_dir/checkpoint_000004, lr=4.706000000000001e-05\n",
            "4: reward: mean=0.28489669007378793, max=0.8994207755109203, min=0.0. length=3.0022539444027045. file=train_dir/checkpoint_000005, lr=4.608e-05\n",
            "5: reward: mean=0.3201371262337052, max=1.0, min=0.0. length=3.2057646116893515. file=train_dir/checkpoint_000006, lr=4.509999999999999e-05\n",
            "6: reward: mean=0.36170124857726427, max=1.0, min=0.0. length=3.4606060606060605. file=train_dir/checkpoint_000007, lr=4.412000000000002e-05\n",
            "7: reward: mean=0.41115074149843245, max=1.0, min=0.0. length=3.768143261074458. file=train_dir/checkpoint_000008, lr=4.3140000000000004e-05\n",
            "8: reward: mean=0.46176041106131654, max=1.0, min=0.0015111152226442057. length=4.175365344467641. file=train_dir/checkpoint_000009, lr=4.216e-05\n",
            "9: reward: mean=0.5313388556570551, max=1.0000000000000002, min=0.0. length=4.794011976047904. file=train_dir/checkpoint_000010, lr=4.1179999999999995e-05\n",
            "10: reward: mean=0.6162608378187655, max=1.0000000000000002, min=0.01072574564769572. length=5.4105691056910565. file=train_dir/checkpoint_000011, lr=4.02e-05\n",
            "11: reward: mean=0.6748667248696409, max=1.0000000000000002, min=0.037418201583084. length=5.902511078286558. file=train_dir/checkpoint_000012, lr=3.922000000000001e-05\n",
            "12: reward: mean=0.7180983178250274, max=1.0000000000000002, min=0.08117061160608177. length=6.278996865203761. file=train_dir/checkpoint_000013, lr=3.8240000000000007e-05\n",
            "13: reward: mean=0.7731755453599457, max=1.0000000000000002, min=0.007209717179153729. length=6.716442953020135. file=train_dir/checkpoint_000014, lr=3.726e-05\n",
            "14: reward: mean=0.8135746410619644, max=1.0000000000000002, min=0.023786911749212283. length=6.979057591623037. file=train_dir/checkpoint_000015, lr=3.627999999999999e-05\n",
            "15: reward: mean=0.8392001213499181, max=1.0, min=0.0541248714848616. length=7.177737881508079. file=train_dir/checkpoint_000016, lr=3.53e-05\n",
            "16: reward: mean=0.8658182995623429, max=1.0000000000000002, min=0.06492120615124143. length=7.606463878326996. file=train_dir/checkpoint_000017, lr=3.4320000000000003e-05\n",
            "17: reward: mean=0.9009312248284924, max=1.0000000000000002, min=0.06689911967185963. length=7.968127490039841. file=train_dir/checkpoint_000018, lr=3.333999999999999e-05\n",
            "18: reward: mean=0.9232838940278356, max=1.0000000000000002, min=0.24805887077045638. length=8.399159663865547. file=train_dir/checkpoint_000019, lr=3.236e-05\n",
            "19: reward: mean=0.9360670045963313, max=1.0000000000000002, min=0.035243682440902334. length=8.465116279069768. file=train_dir/checkpoint_000020, lr=3.1380000000000015e-05\n",
            "20: reward: mean=0.9393732817091394, max=1.0000000000000002, min=0.03445075047870741. length=8.520255863539445. file=train_dir/checkpoint_000021, lr=3.0399999999999994e-05\n",
            "21: reward: mean=0.9607381758109557, max=1.0000000000000002, min=0.2451651769598654. length=8.774122807017545. file=train_dir/checkpoint_000022, lr=2.942e-05\n",
            "22: reward: mean=0.9668669056572442, max=1.0000000000000002, min=0.05776237662456603. length=8.888641425389755. file=train_dir/checkpoint_000023, lr=2.8440000000000002e-05\n",
            "23: reward: mean=0.9739664489776356, max=1.0000000000000002, min=0.7281754157166879. length=8.823788546255507. file=train_dir/checkpoint_000024, lr=2.7459999999999988e-05\n",
            "24: reward: mean=0.983280754103483, max=1.0000000000000002, min=0.24545588346366137. length=9.0. file=train_dir/checkpoint_000025, lr=2.648e-05\n",
            "25: reward: mean=0.9896162185283038, max=1.0000000000000002, min=0.06182073490675758. length=9.139269406392694. file=train_dir/checkpoint_000026, lr=2.5500000000000007e-05\n",
            "26: reward: mean=0.9817107854058453, max=1.0000000000000002, min=0.24541194514023956. length=9.08390022675737. file=train_dir/checkpoint_000027, lr=2.4519999999999992e-05\n",
            "27: reward: mean=0.9833969218405432, max=1.0000000000000002, min=0.08512770773823074. length=9.072562358276643. file=train_dir/checkpoint_000028, lr=2.354e-05\n",
            "28: reward: mean=0.9874787145516031, max=1.0000000000000002, min=0.24545522056619484. length=9.14645308924485. file=train_dir/checkpoint_000029, lr=2.2559999999999994e-05\n",
            "29: reward: mean=0.9908139388778208, max=1.0000000000000002, min=0.41476556490047356. length=9.125570776255708. file=train_dir/checkpoint_000030, lr=2.1579999999999997e-05\n",
            "30: reward: mean=0.9885627225947855, max=1.0000000000000002, min=0.06294517944158842. length=9.111617312072893. file=train_dir/checkpoint_000031, lr=2.0600000000000003e-05\n",
            "31: reward: mean=0.9885392098946905, max=1.0000000000000002, min=0.05895050751632241. length=9.123287671232877. file=train_dir/checkpoint_000032, lr=1.9620000000000002e-05\n",
            "32: reward: mean=0.9952218295952708, max=1.0000000000000002, min=0.4017839393262874. length=9.164759725400458. file=train_dir/checkpoint_000033, lr=1.8639999999999994e-05\n",
            "33: reward: mean=0.9910003019032105, max=1.0000000000000002, min=0.24710660095622022. length=9.136986301369863. file=train_dir/checkpoint_000034, lr=1.7659999999999994e-05\n",
            "34: reward: mean=0.9850556642759335, max=1.0000000000000002, min=0.2466263030559283. length=9.127853881278538. file=train_dir/checkpoint_000035, lr=1.667999999999999e-05\n",
            "35: reward: mean=0.9901185036231621, max=1.0000000000000002, min=0.24512206485164212. length=9.164759725400458. file=train_dir/checkpoint_000036, lr=1.5700000000000002e-05\n",
            "36: reward: mean=0.9947159259086442, max=1.0000000000000002, min=0.03098939755096626. length=9.172413793103448. file=train_dir/checkpoint_000037, lr=1.4719999999999996e-05\n",
            "37: reward: mean=0.992077708503171, max=1.0000000000000002, min=0.5111432986312328. length=9.157534246575343. file=train_dir/checkpoint_000038, lr=1.3739999999999994e-05\n"
          ]
        }
      ]
    }
  ]
}